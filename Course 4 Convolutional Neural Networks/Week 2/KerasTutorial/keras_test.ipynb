{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras import layers\n",
    "from keras.layers import Input, Dense, Activation, ZeroPadding2D, BatchNormalization, Flatten, Conv2D\n",
    "from keras.layers import AveragePooling2D, MaxPooling2D, Dropout, GlobalMaxPooling2D, GlobalAveragePooling2D\n",
    "from keras.models import Model\n",
    "from keras.preprocessing import image\n",
    "from keras.utils import layer_utils\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras.applications.imagenet_utils import preprocess_input\n",
    "import pydot\n",
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "from keras.utils import plot_model\n",
    "from kt_utils import *\n",
    "\n",
    "import keras.backend as K\n",
    "K.set_image_data_format('channels_last')\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import imshow\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of training examples = 600\n",
      "number of test examples = 150\n",
      "X_train shape: (600, 64, 64, 3)\n",
      "Y_train shape: (600, 1)\n",
      "X_test shape: (150, 64, 64, 3)\n",
      "Y_test shape: (150, 1)\n"
     ]
    }
   ],
   "source": [
    "X_train_orig, Y_train_orig, X_test_orig, Y_test_orig, classes = load_dataset()\n",
    "\n",
    "# Normalize image vectors\n",
    "X_train = X_train_orig/255.\n",
    "X_test = X_test_orig/255.\n",
    "\n",
    "# Reshape\n",
    "Y_train = Y_train_orig.T\n",
    "Y_test = Y_test_orig.T\n",
    "\n",
    "print (\"number of training examples = \" + str(X_train.shape[0]))\n",
    "print (\"number of test examples = \" + str(X_test.shape[0]))\n",
    "print (\"X_train shape: \" + str(X_train.shape))\n",
    "print (\"Y_train shape: \" + str(Y_train.shape))\n",
    "print (\"X_test shape: \" + str(X_test.shape))\n",
    "print (\"Y_test shape: \" + str(Y_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: HappyModel\n",
    "\n",
    "def HappyModel(input_shape):\n",
    "    \"\"\"\n",
    "    Implementation of the HappyModel.\n",
    "    \n",
    "    Arguments:\n",
    "    input_shape -- shape of the images of the dataset\n",
    "\n",
    "    Returns:\n",
    "    model -- a Model() instance in Keras\n",
    "    \"\"\"\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "    # Feel free to use the suggested outline in the text above to get started, and run through the whole\n",
    "    # exercise (including the later portions of this notebook) once. The come back also try out other\n",
    "    # network architectures as well. \n",
    "    X_input = Input(input_shape)\n",
    "    X = ZeroPadding2D((1, 1))(X_input)\n",
    "    X = Conv2D(32, (3, 3), strides = (1, 1), name = 'conv0')(X)\n",
    "    X = BatchNormalization(axis = 3, name = 'bn0')(X)\n",
    "    X = Activation('relu')(X)\n",
    "    X = MaxPooling2D((2, 2), name='max_pool0')(X)\n",
    "    X = Conv2D(64, (3, 3), strides = (1, 1), name = 'conv1')(X)\n",
    "    X = Activation('relu')(X)\n",
    "    X = MaxPooling2D((2, 2), name='max_pool1')(X)\n",
    "    X = Flatten()(X)\n",
    "    X = Dense(32, activation='sigmoid', name='fc0')(X)\n",
    "    X = Dense(1, activation='sigmoid', name='fc1')(X)\n",
    "    model = Model(inputs = X_input, outputs = X, name='HappyModel')\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "happyModel = HappyModel((64, 64, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "happyModel.compile(optimizer = \"adam\", loss = 'mean_squared_error', metrics = [\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "600/600 [==============================] - 11s 18ms/step - loss: 0.3339 - acc: 0.5267\n",
      "Epoch 2/50\n",
      "600/600 [==============================] - 10s 17ms/step - loss: 0.1816 - acc: 0.7700\n",
      "Epoch 3/50\n",
      "600/600 [==============================] - 10s 17ms/step - loss: 0.1135 - acc: 0.8967\n",
      "Epoch 4/50\n",
      "600/600 [==============================] - 10s 17ms/step - loss: 0.0607 - acc: 0.9550\n",
      "Epoch 5/50\n",
      "600/600 [==============================] - 11s 18ms/step - loss: 0.0435 - acc: 0.9650\n",
      "Epoch 6/50\n",
      "600/600 [==============================] - 10s 17ms/step - loss: 0.0328 - acc: 0.9617\n",
      "Epoch 7/50\n",
      "600/600 [==============================] - 10s 17ms/step - loss: 0.0237 - acc: 0.9733\n",
      "Epoch 8/50\n",
      "600/600 [==============================] - 10s 17ms/step - loss: 0.0188 - acc: 0.9817\n",
      "Epoch 9/50\n",
      "600/600 [==============================] - 10s 17ms/step - loss: 0.0152 - acc: 0.9867\n",
      "Epoch 10/50\n",
      "600/600 [==============================] - 10s 17ms/step - loss: 0.0117 - acc: 0.9917\n",
      "Epoch 11/50\n",
      "600/600 [==============================] - 10s 17ms/step - loss: 0.0155 - acc: 0.9850\n",
      "Epoch 12/50\n",
      "600/600 [==============================] - 10s 17ms/step - loss: 0.0117 - acc: 0.9850\n",
      "Epoch 13/50\n",
      "600/600 [==============================] - 10s 17ms/step - loss: 0.0151 - acc: 0.9817\n",
      "Epoch 14/50\n",
      "600/600 [==============================] - 10s 17ms/step - loss: 0.0114 - acc: 0.9867\n",
      "Epoch 15/50\n",
      "600/600 [==============================] - 10s 17ms/step - loss: 0.0105 - acc: 0.9917\n",
      "Epoch 16/50\n",
      "600/600 [==============================] - 10s 17ms/step - loss: 0.0084 - acc: 0.9933\n",
      "Epoch 17/50\n",
      "600/600 [==============================] - 10s 17ms/step - loss: 0.0072 - acc: 0.9933\n",
      "Epoch 18/50\n",
      "600/600 [==============================] - 10s 17ms/step - loss: 0.0066 - acc: 0.9933\n",
      "Epoch 19/50\n",
      "600/600 [==============================] - 10s 17ms/step - loss: 0.0073 - acc: 0.9917\n",
      "Epoch 20/50\n",
      "600/600 [==============================] - 10s 17ms/step - loss: 0.0056 - acc: 0.9950\n",
      "Epoch 21/50\n",
      "600/600 [==============================] - 10s 17ms/step - loss: 0.0079 - acc: 0.9933\n",
      "Epoch 22/50\n",
      "600/600 [==============================] - 11s 18ms/step - loss: 0.0110 - acc: 0.9867\n",
      "Epoch 23/50\n",
      "600/600 [==============================] - 11s 18ms/step - loss: 0.0105 - acc: 0.9883\n",
      "Epoch 24/50\n",
      "600/600 [==============================] - 10s 17ms/step - loss: 0.0070 - acc: 0.9933\n",
      "Epoch 25/50\n",
      "600/600 [==============================] - 10s 17ms/step - loss: 0.0047 - acc: 0.9950\n",
      "Epoch 26/50\n",
      "600/600 [==============================] - 11s 19ms/step - loss: 0.0096 - acc: 0.9867\n",
      "Epoch 27/50\n",
      "600/600 [==============================] - 11s 18ms/step - loss: 0.0059 - acc: 0.9933\n",
      "Epoch 28/50\n",
      "600/600 [==============================] - 11s 18ms/step - loss: 0.0037 - acc: 0.9967\n",
      "Epoch 29/50\n",
      "600/600 [==============================] - 10s 17ms/step - loss: 0.0037 - acc: 0.9967\n",
      "Epoch 30/50\n",
      "600/600 [==============================] - 11s 18ms/step - loss: 0.0024 - acc: 0.9983\n",
      "Epoch 31/50\n",
      "600/600 [==============================] - 11s 18ms/step - loss: 0.0051 - acc: 0.9917\n",
      "Epoch 32/50\n",
      "600/600 [==============================] - 11s 18ms/step - loss: 0.0038 - acc: 0.9933\n",
      "Epoch 33/50\n",
      "600/600 [==============================] - 11s 18ms/step - loss: 0.0022 - acc: 0.9983\n",
      "Epoch 34/50\n",
      "600/600 [==============================] - 11s 18ms/step - loss: 0.0020 - acc: 0.9983\n",
      "Epoch 35/50\n",
      "600/600 [==============================] - 14s 24ms/step - loss: 0.0019 - acc: 0.9983\n",
      "Epoch 36/50\n",
      "600/600 [==============================] - 11s 19ms/step - loss: 0.0019 - acc: 0.9983\n",
      "Epoch 37/50\n",
      "600/600 [==============================] - 11s 19ms/step - loss: 0.0018 - acc: 0.9983\n",
      "Epoch 38/50\n",
      "600/600 [==============================] - 11s 18ms/step - loss: 0.0018 - acc: 0.9983\n",
      "Epoch 39/50\n",
      "600/600 [==============================] - 11s 19ms/step - loss: 0.0017 - acc: 0.9983\n",
      "Epoch 40/50\n",
      "600/600 [==============================] - 11s 18ms/step - loss: 0.0018 - acc: 0.9983\n",
      "Epoch 41/50\n",
      "600/600 [==============================] - 11s 18ms/step - loss: 0.0017 - acc: 0.9983\n",
      "Epoch 42/50\n",
      "600/600 [==============================] - 11s 18ms/step - loss: 0.0017 - acc: 0.9983\n",
      "Epoch 43/50\n",
      "600/600 [==============================] - 12s 20ms/step - loss: 0.0019 - acc: 0.9983\n",
      "Epoch 44/50\n",
      "600/600 [==============================] - 12s 20ms/step - loss: 0.0017 - acc: 0.9983\n",
      "Epoch 45/50\n",
      "600/600 [==============================] - 11s 19ms/step - loss: 0.0020 - acc: 0.9983\n",
      "Epoch 46/50\n",
      "600/600 [==============================] - 12s 20ms/step - loss: 0.0024 - acc: 0.9983\n",
      "Epoch 47/50\n",
      "600/600 [==============================] - 13s 21ms/step - loss: 0.0016 - acc: 0.9983\n",
      "Epoch 48/50\n",
      "600/600 [==============================] - 12s 20ms/step - loss: 0.0017 - acc: 0.9983\n",
      "Epoch 49/50\n",
      "600/600 [==============================] - 14s 23ms/step - loss: 0.0017 - acc: 0.9983\n",
      "Epoch 50/50\n",
      "600/600 [==============================] - 12s 20ms/step - loss: 0.0014 - acc: 0.9983\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f6b31523f98>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "happyModel.fit(x = X_train, y = Y_train, epochs = 50, batch_size = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150/150 [==============================] - 1s 9ms/step\n",
      "\n",
      "Loss = 0.0329162518339\n",
      "Test Accuracy = 0.960000003974\n"
     ]
    }
   ],
   "source": [
    "preds = happyModel.evaluate(X_test, Y_test)\n",
    "print()\n",
    "print (\"Loss = \" + str(preds[0]))\n",
    "print (\"Test Accuracy = \" + str(preds[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
